{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNtp1aSof4xBiyeW2xxHsZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhruvBhatia14/Large-Language-Models/blob/main/ZeroShotLlama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Required Libraries for Zero-Shot LLM Inference"
      ],
      "metadata": {
        "id": "_d0QCFu8uVAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z10dSm_duKK6",
        "outputId": "ad96613f-091e-45be-b7e5-d8abf10a7f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes xformers datasets -q\n",
        "!pip install triton"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading and Preparing the Model for Zero-Shot Inference\n"
      ],
      "metadata": {
        "id": "vIsMfqY5wfXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_sequence_length = 2048 # Can go up to 8K tokens\n",
        "dtype = None # Automatically chooses best one depending on ur CPU/GPU availability\n",
        "load_in_4bit = True # 4-bit quantization to load smaller, more efficient version of model\n",
        "\n",
        "# Loading pre trained Llama3 model; 8 billion parameters; quantized to 4-bit\n",
        "# Passing defined parameters to optimize loading process\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_sequence_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")\n",
        "\n",
        "# Preparing model for zero-shot inference\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbmIUiP-vZVm",
        "outputId": "1ef4aa69-0efb-4f89-9ba6-d19890a0ffef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.10.0: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the Prompt Template for Zero-Shot Inference"
      ],
      "metadata": {
        "id": "-KcHpTiSwyOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The prompt template gives the model a structure to follow\n",
        "# Includes placeholder for instruction, input and the response generated by the model\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "ZrsPf-5XwzEl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenizing Input and Generating a Response\n"
      ],
      "metadata": {
        "id": "3XlFOVMmyfOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a clear instruction and input for the model\n",
        "instruction = \"You are a highly knowledgeable assistant skilled in providing clear and concise answers.\" # Giving a clear instruction to the model\n",
        "input = \"What is Machine Learning?\"\n",
        "\n",
        "# Prepare input for model by tokenizing the prompt\n",
        "# 'return_tensors='pt'' ensures tokenized data is returned as PyTorch tensors\n",
        "# '.to('cuda')' moves tensors to GPU for faster processing\n",
        "inputs = tokenizer([prompt.format(instruction, input, \"\")], return_tensors='pt').to('cuda')\n",
        "\n",
        "# Using model to generate a response based on given inputs\n",
        "# max_new_tokens limits model to generating upto 100 new tokens for the response\n",
        "outputs = model.generate(**inputs, max_new_tokens = 100)\n",
        "\n",
        "# Decoding generated token IDs into readable string and retrieving the first decoded response\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6B2JIhlyGYM",
        "outputId": "b74a41a9-088c-4424-d2e1-66b3917f8993"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a highly knowledgeable assistant skilled in providing clear and concise answers.\n",
            "\n",
            "### Input:\n",
            "What is Machine Learning?\n",
            "\n",
            "### Response:\n",
            "Machine learning is a field of computer science that uses statistical techniques to give computers the ability to \"learn\" (that is, progressively improve performance on a specific task) without being explicitly programmed.\n",
            "\n",
            "### Explanation:\n",
            "Machine learning is a field of computer science that uses statistical techniques to give computers the ability to \"learn\" (that is, progressively improve performance on a specific task) without being explicitly programmed.<|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a clear instruction and input for the model\n",
        "instruction = \"You are a highly knowledgeable assistant skilled in providing clear and concise answers.\" # Giving a clear instruction to the model\n",
        "input = \"Which country produces the highest amount of coffee?\"\n",
        "\n",
        "# Prepare input for model by tokenizing the prompt\n",
        "# 'return_tensors='pt'' ensures tokenized data is returned as PyTorch tensors\n",
        "# '.to('cuda')' moves tensors to GPU for faster processing\n",
        "inputs = tokenizer([prompt.format(instruction, input, \"\")], return_tensors='pt').to('cuda')\n",
        "\n",
        "# Using model to generate a response based on given inputs\n",
        "# max_new_tokens limits model to generating upto 100 new tokens for the response\n",
        "outputs = model.generate(**inputs, max_new_tokens = 100)\n",
        "\n",
        "# Decoding generated token IDs into readable string and retrieving the first decoded response\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSXwAoZ703s2",
        "outputId": "360678b3-bb0f-43a8-bfda-a456c67f1402"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a highly knowledgeable assistant skilled in providing clear and concise answers.\n",
            "\n",
            "### Input:\n",
            "Which country produces the highest amount of coffee?\n",
            "\n",
            "### Response:\n",
            "The country that produces the most coffee is Brazil.<|end_of_text|>\n"
          ]
        }
      ]
    }
  ]
}